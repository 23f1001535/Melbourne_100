{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fVwsNGQRcAW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Replace with your GitHub personal access token\n",
        "token = \"Enter your token\"\n",
        "HEADERS = {'Authorization': f'token {TOKEN}'}\n",
        "\n",
        "def get_users_by_city(city, min_followers):\n",
        "    url = f'https://api.github.com/search/users?q=location:melbourne+followers:>100'\n",
        "    users = []\n",
        "    page = 1\n",
        "    per_page = 30\n",
        "\n",
        "    while True:\n",
        "        # Add pagination parameters to the URL\n",
        "        paginated_url = f\"{url}&page={page}&per_page={per_page}\"\n",
        "        response = requests.get(paginated_url, headers=HEADERS)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            items = data.get('items', [])\n",
        "            if not items:\n",
        "                break  # No more results\n",
        "\n",
        "            users.extend(items)\n",
        "\n",
        "            # If less than the max items per page, we are likely on the last page\n",
        "            if len(items) < per_page:\n",
        "                break\n",
        "\n",
        "            page += 1  # Increment page for the next request\n",
        "            time.sleep(1)  # Optional: pause to avoid hitting rate limits\n",
        "\n",
        "        else:\n",
        "            print(f'Error: {response.status_code}, {response.text}')\n",
        "            break\n",
        "\n",
        "    return users\n",
        "\n",
        "def save_users_to_csv(users, filename):\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['login', 'name', 'company', 'location', 'email', 'bio', 'followers', 'public_repos', 'hireable', 'created_at','following']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for user in users:\n",
        "            # Fetch additional details for each user\n",
        "            user_details_response = requests.get(user['url'], headers=HEADERS)\n",
        "            if user_details_response.status_code == 200:\n",
        "                user_details = user_details_response.json()\n",
        "                writer.writerow({\n",
        "                    'login': user_details.get('login'),\n",
        "                    'name': user_details.get('name'),\n",
        "                    'company': user_details.get('company'),\n",
        "                    'location': user_details.get('location'),\n",
        "                    'email': user_details.get('email'),\n",
        "                    'bio': user_details.get('bio'),\n",
        "                    'followers': user_details.get('followers'),\n",
        "                    'following':user_details.get('following'),\n",
        "                    'public_repos': user_details.get('public_repos'),\n",
        "                    'hireable': user_details.get('hireable'),\n",
        "                    'created_at': user_details.get('created_at')\n",
        "                })\n",
        "                time.sleep(0.5)  # Optional: pause between user details requests to avoid rate limiting\n",
        "            else:\n",
        "                print(f\"Failed to fetch details for {user['login']}: {user_details_response.status_code}\")\n",
        "\n",
        "# Example usage\n",
        "city = 'melbourne'\n",
        "min_followers = 100\n",
        "users = get_users_by_city(city, min_followers)\n",
        "save_users_to_csv(users, 'users.csv')\n",
        "print(\"Data saved to users.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/users.csv\")\n",
        "lt = df['login'].unique().tolist()\n",
        "len(lt)"
      ],
      "metadata": {
        "id": "qNu8nUs8RdP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Use your GitHub token here for a higher rate limit\n",
        "token = \"Enter your token\"\n",
        "headers = {'Authorization': f'token {token}'}\n",
        "\n",
        "def get_all_repos(username):\n",
        "    \"\"\"\n",
        "    Fetches all public repositories for a GitHub user, handling pagination.\n",
        "    \"\"\"\n",
        "    all_repos = []\n",
        "    page = 1\n",
        "\n",
        "    while True:\n",
        "        url = f'https://api.github.com/users/{username}/repos?page={page}&per_page=100'\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch data for {username}: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        repos = response.json()\n",
        "        if not repos:\n",
        "            break  # Exit the loop if no more repos are found\n",
        "\n",
        "        all_repos.extend(repos)\n",
        "        page += 1\n",
        "        time.sleep(0.5)  # Optional: Throttle requests to avoid hitting the rate limit\n",
        "\n",
        "    return all_repos\n",
        "\n",
        "def save_limited_repos_to_csv(usernames, filename='repositories.csv'):\n",
        "    \"\"\"\n",
        "    Fetches repositories for multiple users and saves up to 500 entries per user\n",
        "    (sorted by `pushed_at` for users with more than 500 repositories).\n",
        "    \"\"\"\n",
        "    all_repos_data = []\n",
        "\n",
        "    for username in usernames:\n",
        "        repos = get_all_repos(username)\n",
        "\n",
        "        # Apply sorting ONLY if user has more than 500 repositories\n",
        "        if len(repos) > 500:\n",
        "            repos_sorted = sorted(\n",
        "                repos,\n",
        "                key=lambda x: datetime.strptime(x['pushed_at'], '%Y-%m-%dT%H:%M:%SZ'),\n",
        "                reverse=True\n",
        "            )\n",
        "            repos = repos_sorted[:500]  # Take the most recent 500 repositories\n",
        "\n",
        "        # Process repository data\n",
        "        for repo in repos:\n",
        "            all_repos_data.append({\n",
        "                'username': username,\n",
        "                'name': repo.get('name'),\n",
        "                'full_name': repo.get('full_name'),\n",
        "                'private': repo.get('private'),\n",
        "                'html_url': repo.get('html_url'),\n",
        "                'description': repo.get('description'),\n",
        "                'fork': repo.get('fork'),\n",
        "                'created_at': repo.get('created_at'),\n",
        "                'updated_at': repo.get('updated_at'),\n",
        "                'pushed_at': repo.get('pushed_at'),\n",
        "                'stargazers_count': repo.get('stargazers_count'),\n",
        "                'watchers_count': repo.get('watchers_count'),\n",
        "                'language': repo.get('language'),\n",
        "                'has_issues': repo.get('has_issues'),\n",
        "                'has_projects': repo.get('has_projects'),\n",
        "                'has_wiki': repo.get('has_wiki'),\n",
        "                'license': repo.get('license', {}).get('key') if repo.get('license') else None,\n",
        "                'forks_count': repo.get('forks_count')\n",
        "            })\n",
        "        print(f\"Processed repositories for {username} (Total: {len(repos)})\")\n",
        "\n",
        "    # Convert list of all repos to DataFrame and save\n",
        "    df = pd.DataFrame(all_repos_data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "usernames = lt\n",
        "save_limited_repos_to_csv(usernames)"
      ],
      "metadata": {
        "id": "eqKhZcQfRf7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
